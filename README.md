# Structure

.

├── Dockerfile

├── README.md

├── dataset

├── infer.py

├── metatree_model

│   ├── README.md

│   ├── config.json

│   └── generation_config.json

├── model_initialize

│   └── main.py

├── requirements.txt

└── train.py

4 directories, 9 files

# malicious-url-detection
MIPT MLOps project. Focusing on malicious URL detection.

Based on dataset (Kaggle): https://www.kaggle.com/datasets/pilarpieiro/tabular-dataset-ready-for-malicious-url-detection/data

## Формулировка задачи

Задача состоит в обнаружении вредоносных URL-адресов по тексту самой ссылки. Подобная задача бинарной классификации очень полезна для исследований в области кибербезопасности и обнаружения угроз. Используемый датасет предлагает довольно большое количество фичей (таких, как энтропия, данные о происхождении URL'а, его параметры), что дает почву для изучения новых подходов к повышению точности и надежности подобных систем классификации. Кроме того, решение такой задачи способствует обмену знаниями внутри сообщества кибербезопасности.

## Данные

Источник (датасет): https://www.kaggle.com/datasets/pilarpieiro/tabular-dataset-ready-for-malicious-url-detection/data

У данных есть следующие особенности:

- 60 полей: 48 типа int64, 9 типа float64 и 3 текстового типа

- суммарно в двух .csv файлах содержится 6.603.969 "чистых" записей и 1.807.092 записей, помеченных "подозрительными"

- большое количество заранее подготовленных данных о вхождениях ключевых слов, математических функциях от текста URL'а, подсчет различных символов и другие свойства интернет-ссылок

- проблемой может стать определение того, какие фичи и свойства являются существенными, а какие нет. Также при добавлении новых данных в датасет следует произвести вычисления необходимых параметров

- выборка достаточно обширна для качественного обучения модели, так что такого объема данных вполне может хватить для решения поставленной задачи

## Подход к моделированию

Предполагаемые для использования Python-библиотеки: NumPy, Pandas, Seaborn, Matplotlib, PyTorch (возможно).

Касательно предполагаемой для использования нейросети: необходима модель для классификации табличных данных. На данный момент я остановился на https://huggingface.co/keras-io/tab_transformer , однако конкретная модель при реализации проекта может измениться.

##  Способ предсказания (инференс)

Необходимые шаги для Production Pipeline (CI):

- Data extraction

- Data validation

- Data preparation

- Model training

- Model evaluation

- Model validation

Шаг Continuous Delivery может содержать следующие этапы:

- проверка совместимости модели с целевой инфраструктурой перед развертыванием (проверка установки пакетов, доступ к ресурсам)

- тестирование службы прогнозирования путем вызова API службы с ожидаемыми входными данными и проверки получения ожидаемого ответа

- нагрузочное тестирование службы прогнозирования, сбор количества запросов в секунду (QPS/RPS) и подсчет задержки вывода

- проверка данных либо для повторного обучения, либо для batch prediction

- проверка соответствия модеи прогнозируемым показателям производительности перед развертыванием

- автоматическое развертывание в тестовой среде

- полуавтоматическое развертывание в препроде

- развертывание вручную в проде
